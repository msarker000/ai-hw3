{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will look at a typical image based machine learning task.\n",
    "\n",
    "## Image classification \n",
    "\n",
    "For this task the whole image is used to classify what's happening.\n",
    "\n",
    "For this specific task, we will be trying to classify COVID-19 using pneumonia x-rays.  Please note, the literature has mostly suggested CT scans are not an effective way of figuring out what type of disease you have.  This exercise is for academic purposes _only_.\n",
    "\n",
    "Steps:\n",
    "\n",
    "\n",
    "1. Download the pneumonia data.  \n",
    "\n",
    "You can find it here:\n",
    "\n",
    "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "move the folder to this directory and unzip it.  Please don't change any folder names or the below script will not work.  Also make sure the folder is in the same directory as this notebook!\n",
    "\n",
    "2. load the pneumonia data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_training_data():\n",
    "    paths = [\n",
    "        \"chest_xray/train/NORMAL/*\",\n",
    "        \"chest_xray/train/PNEUMONIA/*\"\n",
    "    ]\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for path in paths:\n",
    "        for im_path in glob.glob(path):\n",
    "            if path == \"chest_xray/train/NORMAL/*\":\n",
    "                labels.append(\"NORMAL\")\n",
    "            if path == \"chest_xray/train/PNEUMONIA/*\":\n",
    "                labels.append(\"PNEUMONIA\")\n",
    "            image_paths.append(im_path)\n",
    "    return image_paths, labels\n",
    "\n",
    "def load_testing_data():\n",
    "    paths = [\n",
    "        \"chest_xray/test/NORMAL/*\",\n",
    "        \"chest_xray/test/PNEUMONIA/*\"\n",
    "    ]\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for path in paths:\n",
    "        for im_path in glob.glob(path):\n",
    "            if path == \"chest_xray/test/NORMAL/*\":\n",
    "                labels.append(\"NORMAL\")\n",
    "            if path == \"chest_xray/test/PNEUMONIA/*\":\n",
    "                labels.append(\"PNEUMONIA\")\n",
    "            image_paths.append(im_path)\n",
    "    return image_paths, labels\n",
    "\n",
    "train_paths, train_labels = load_training_data()\n",
    "test_paths, test_labels = load_testing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. read the data into memory, I recommend open-cv for this:\n",
    "\n",
    "`python -m pip install opencv-python` \n",
    "\n",
    "if you don't already have it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "train_images = load_images(train_paths)\n",
    "test_images = load_images(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. resize the images to a standard size - \n",
    "\n",
    "Note: it ought to be a box.  So the width and height should be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resize_images(images):\n",
    "    imgs = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "train_images = resize_images(train_images)\n",
    "test_images = resize_images(test_images)\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Greyscale the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greyscale_images(images):\n",
    "    gray_imgs = []\n",
    "    for img in images:\n",
    "       # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = img\n",
    "        gray_imgs.append(gray)\n",
    "    return gray_imgs\n",
    "\n",
    "train_images = greyscale_images(train_images)\n",
    "test_images = greyscale_images(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. prepare the data for training the model.\n",
    "\n",
    "For this you'll need to transform the test and train image objects into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def features_to_np_array(images):\n",
    "    return np.array(images)\n",
    "\n",
    "train_images = features_to_np_array(train_images)\n",
    "test_images = features_to_np_array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll need to do the same for the labels:\n",
    "\n",
    "Note: You'll need to apply the `to_categorical` function after transforming to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "LABELS = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "\n",
    "def labels_to_np_array(labels):\n",
    "    Y = np.array([LABELS[l] for l in labels])\n",
    "    return to_categorical(Y, len(LABELS))\n",
    "\n",
    "train_labels = labels_to_np_array(train_labels)\n",
    "test_labels = labels_to_np_array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Seperate into train and test with `train_test_split` from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split code goes here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val=train_test_split(train_images, train_labels, test_size=0.23, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Make the last four layers of VGG16 with imagenet weights trainable and then retrain the model.\n",
    "\n",
    "To understand how to do this, please see the following tutorial:\n",
    "\n",
    "https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ayub/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ayub/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4016 samples, validate on 1200 samples\n",
      "Epoch 1/5\n",
      "4016/4016 [==============================] - 305s 76ms/step - loss: 4.0859 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 2/5\n",
      "4016/4016 [==============================] - 320s 80ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 3/5\n",
      "4016/4016 [==============================] - 294s 73ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 4/5\n",
      "4016/4016 [==============================] - 294s 73ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 5/5\n",
      "4016/4016 [==============================] - 294s 73ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x45448ab00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "EPOCHS = 5\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "#model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_val, y_val), verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Check your score with classification_report from scikit-learn\n",
    "\n",
    "Now that you've trained your model, call `model.predict` to get the predicted values for classification.  \n",
    "Then compare your predicted values with y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 28s 45ms/step\n",
      "Test accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "loss , accuracy = model.evaluate(test_images , test_labels , batch_size = 32)\n",
    "print('Test accuracy: {:2.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.00      0.00      0.00       234\n",
      "   PNEUMONIA       0.62      1.00      0.77       390\n",
      "\n",
      "    accuracy                           0.62       624\n",
      "   macro avg       0.31      0.50      0.38       624\n",
      "weighted avg       0.39      0.62      0.48       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classification report code goes here.\n",
    "y_pred = model.predict(test_images)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(test_labels.argmax(axis=1), y_pred, target_names=LABELS.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Data augmentation\n",
    "\n",
    "Now that you have a classifier, let's see if data augmentation improves things!  \n",
    "\n",
    "You can use the `ImageDataGenerator` that comes with keras.  Here's how to import it:\n",
    "\n",
    "`from tensorflow.keras.preprocessing.image import ImageDataGenerator`\n",
    "\n",
    "Here's the documentation: https://keras.io/preprocessing/image/\n",
    "\n",
    "Here's an example of it getting used in the wild, in case you get stuck:\n",
    "\n",
    "https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment your data here\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "image_data_generator = ImageDataGenerator(rotation_range=20, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. retrain your classifier\n",
    "\n",
    "Now that you have augmented training data, please retrain your classifier.  The code should basically be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "502/502 [==============================] - 427s 851ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 2/5\n",
      "502/502 [==============================] - 404s 804ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 3/5\n",
      "502/502 [==============================] - 403s 803ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 4/5\n",
      "502/502 [==============================] - 407s 812ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 5/5\n",
      "502/502 [==============================] - 426s 848ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "# new training code goes here\n",
    "H = model.fit_generator(image_data_generator.flow(X_train, y_train, batch_size=BS),\n",
    "                        steps_per_epoch=len(X_train) // BS,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        validation_steps=len(X_val) // BS,\n",
    "                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. re-evaluate your classifier\n",
    "\n",
    "Now that you've augmented the data, please re-evaluate your classifer.  Use classification report like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 29s 46ms/step\n",
      "Test accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "loss , accuracy = model.evaluate(test_images , test_labels , batch_size = 32)\n",
    "print('Test accuracy: {:2.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.00      0.00      0.00       234\n",
      "   PNEUMONIA       0.62      1.00      0.77       390\n",
      "\n",
      "    accuracy                           0.62       624\n",
      "   macro avg       0.31      0.50      0.38       624\n",
      "weighted avg       0.39      0.62      0.48       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# classification report goes here\n",
    "y_pred = model.predict(test_images)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(test_labels.argmax(axis=1), y_pred, target_names=LABELS.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Evaluate the difference with data augmentation and without:\n",
    "\n",
    "Did things improve?  Did they stay the same?  Did they get worse?  Please try to come up with an explanation of why you got the results you did.\n",
    "\n",
    "No Improvement. They are same. May be because of the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of results go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Getting COVID19 data\n",
    "\n",
    "Now that you have a trained classifier with pneumonia, we are going to use this with COVID data.  \n",
    "\n",
    "Clone this repo:\n",
    "\n",
    "https://github.com/ieee8023/covid-chestxray-dataset\n",
    "\n",
    "use the clone command: `git clone [REPO]`\n",
    "\n",
    "to get the data locally.  \n",
    "\n",
    "Make sure to run this command in the same folder as this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Read the data into memory\n",
    "\n",
    "The set up for this data repository is a little different.  Please use the following code to read the data into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_covid19():\n",
    "    base = \"covid-chestxray-dataset/\"\n",
    "    metadata = pd.read_csv(base+\"metadata.csv\")\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for index, row in metadata.iterrows():\n",
    "        labels.append(row[\"finding\"])\n",
    "        image_paths.append(base+row[\"filename\"])\n",
    "    return labels, image_paths\n",
    "\n",
    "labels, covid_image_paths = get_covid19()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. preprocess images\n",
    "\n",
    "you'll need to run the following functions on this data:\n",
    "\n",
    "1. load_images\n",
    "2. resize_images\n",
    "3. greyscale_images\n",
    "4. features_to_np_array\n",
    "5. labels_to_np_array\n",
    "\n",
    "Make sure to run each of those functions in order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your function calls to covid_image_paths here\n",
    "covid_images = load_images(covid_image_paths)\n",
    "covid_images = resize_images(covid_images)\n",
    "covid_images = greyscale_images(covid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Strip out labels other than 'No Finding' and 'COVID-19' from the dataset\n",
    "\n",
    "There are two straight forward ways to do this:\n",
    "\n",
    "1) use a for-loop and keep track of indices\n",
    "\n",
    "2) read labels and features into a dataframe and then filter to those two label types.  Your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label reduction code goes here\n",
    "filtered_labels = []\n",
    "for l in labels:\n",
    "    if l == 'COVID-19' or l == 'No Finding':\n",
    "        filtered_labels.append(l)\n",
    "\n",
    "labels = filtered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Predict on the new images\n",
    "\n",
    "Here you'll use the classifier you trained on just pneumonia/not pneumonia to try and classify COVID-19 and no finding.  You'll use the pneumonia/not pneumonia classifier as a featurizer to do this.\n",
    "\n",
    "Much of the code has been written, you'll just need to supply your trained classifier as input.\n",
    "\n",
    "Please predict the labels from the classifier.  Then run `classification_report` to see how well your classifier did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       234\n",
      "           1       0.53      1.00      0.69       262\n",
      "\n",
      "    accuracy                           0.53       496\n",
      "   macro avg       0.26      0.50      0.35       496\n",
      "weighted avg       0.28      0.53      0.37       496\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#prediction code goes here\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import glob\n",
    "import code\n",
    "\n",
    "def extract_features_covid(model, width, height):\n",
    "    base = \"covid-chestxray-dataset/\"\n",
    "    metadata = pd.read_csv(base+\"metadata.csv\")\n",
    "    labels = []\n",
    "    feature_list = []\n",
    "    for index, row in metadata.iterrows():\n",
    "        if row[\"finding\"] == \"COVID-19\":\n",
    "            labels.append(\"COVID\")\n",
    "            im_path = base+\"images/\"+row[\"filename\"]\n",
    "            im = cv2.imread(im_path, cv2.IMREAD_COLOR)\n",
    "            if im is None:\n",
    "                continue\n",
    "            im = cv2.resize(im, (width, height))\n",
    "            features = model.predict(im.reshape(1,width,height,3))\n",
    "            features_np = np.array(features)\n",
    "            feature_list.append(features_np.flatten())\n",
    "\n",
    "    return np.array(feature_list), labels\n",
    "\n",
    "def extract_features_not_covid(model, width, height):\n",
    "    feature_list = []\n",
    "    labels = []\n",
    "    paths = [\n",
    "        \"chest_xray/test/NORMAL/*\",\n",
    "        \"chest_xray/test/PNEUMONIA/*\",\n",
    "        \"chest_xray/train/NORMAL/*\",\n",
    "        \"chest_xray/train/PNEUMONIA/*\"\n",
    "        \n",
    "    ]\n",
    "    for path in paths:\n",
    "        for im_path in glob.glob(path):\n",
    "            if path == \"chest_xray/train/NORMAL/*\":\n",
    "                labels.append(\"CLEAR TRAIN\")\n",
    "            if path == \"chest_xray/test/NORMAL/*\":\n",
    "                labels.append(\"CLEAR TEST\")\n",
    "            if path == \"chest_xray/train/PNEUMONIA/*\":\n",
    "                labels.append(\"PNEUMONIA\")\n",
    "            im = cv2.imread(im_path, cv2.IMREAD_COLOR)\n",
    "            im = cv2.resize(im, (width, height))\n",
    "            features = model.predict(im.reshape(1,width,height,3))\n",
    "            features_np = np.array(features)\n",
    "            feature_list.append(features_np.flatten())\n",
    "\n",
    "    return np.array(feature_list), labels\n",
    "\n",
    "# please make a copy of your tuned model and save it to variable:\n",
    "untuned_model = model\n",
    "\n",
    "# please specify the width and height you used for the image preprocessing\n",
    "width =64\n",
    "height = 64\n",
    "\n",
    "covid_features, covid_labels = extract_features_covid(untuned_model, width, height)\n",
    "non_covid_features, non_covid_labels = extract_features_not_covid(untuned_model, width, height)\n",
    "features =  np.concatenate((covid_features, non_covid_features), axis=0)\n",
    "labels =  np.concatenate((covid_labels, non_covid_labels), axis=0)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index, label in enumerate(labels):\n",
    "    if label == \"CLEAR TRAIN\":\n",
    "        X_train.append(features[index])\n",
    "        y_train.append(0)\n",
    "    if label == \"PNEUMONIA\":\n",
    "        X_train.append(features[index])\n",
    "        y_train.append(1)\n",
    "    if label == \"COVID\":\n",
    "        X_test.append(features[index])\n",
    "        y_test.append(1)\n",
    "    if label == \"CLEAR TEST\":\n",
    "        X_test.append(features[index])\n",
    "        y_test.append(0)\n",
    "\n",
    "logit_clf = LogisticRegression()\n",
    "logit_clf.fit(X_train, y_train)\n",
    "y_pred = logit_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Compare and contrast how the classifier did on Pneumonia versus COVID-19\n",
    "\n",
    "Did it do as well?  Worse?  About the same?  What conclusions can you draw?\n",
    "\n",
    "\n",
    "Answer: it get worse here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your answers here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at a bunch of base cases, let's see if we can improve things by changing the model architecture.  We'll do this with a bunch of discrete steps\n",
    "\n",
    "1. Change the number of trainable layers\n",
    "\n",
    "Here you will make more of the layers trainable.  For this we are going to use cross validation to try and figure out which the optimal number of trainable layers.  Please us from the last 6 layers to one layer.  So your range should be:\n",
    "\n",
    "```\n",
    "trainable_range = [-6, -5, -4, -3, -2, -1]\n",
    "```\n",
    "\n",
    "Also, your X and y data should be the pneumonia data only.  Since that's what we trained on.  We should not assume we have access to the COVID data, except for testing, which will do later on.\n",
    "\n",
    "Here's a blog post detailing how to set this up: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "Note you'll need to set the number of trainable layers inside of `model_create` in order to make this tunable.  \n",
    "\n",
    "Please report mean and standard deviation for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5216/5216 [==============================] - 266s 51ms/step - loss: 2.2830 - accuracy: 0.8541\n",
      "Best: 0.685445 using {'trainable_layer_range': -1}\n",
      "0.076289 (0.107889) with: {'trainable_layer_range': -6}\n",
      "0.409622 (0.427788) with: {'trainable_layer_range': -5}\n",
      "0.409622 (0.427788) with: {'trainable_layer_range': -4}\n",
      "0.409622 (0.427788) with: {'trainable_layer_range': -3}\n",
      "0.409622 (0.427788) with: {'trainable_layer_range': -2}\n",
      "0.685445 (0.325541) with: {'trainable_layer_range': -1}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Cross validation code goes here\n",
    "def create_model(trainable_layer_range=-6):\n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for layer in vgg_conv.layers[:trainable_layer_range]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    "\n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "kerasCl_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "trainable_range = [-6, -5, -4, -3, -2, -1]\n",
    "\n",
    "param_grid = dict(trainable_layer_range=trainable_range)\n",
    "grid = GridSearchCV(estimator=kerasCl_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(train_images, train_labels)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyze your results\n",
    "\n",
    "Do you think that changing the number of tunable layers matters?  Does it improve classification accuracy enough to warrant changing the number of tunable layers?\n",
    "\n",
    "\n",
    "Analysis: Yes. A little bit improvement in trainable layer with -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and explanation go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tune over a layer activation function\n",
    "\n",
    "Please set the number of tunable layers to 4 again.\n",
    "\n",
    "Now we are going to make the layer activation tunable.  \n",
    "\n",
    "To do this, please change the model_create function so that each layer has it's own tunable activation function.  Then run your new cross validation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5216/5216 [==============================] - 324s 62ms/step - loss: 0.6318 - accuracy: 0.7414\n",
      "Best: 0.742956 using {'activation': 'softmax'}\n",
      "0.742956 (0.363516) with: {'activation': 'softmax'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu'}\n",
      "0.409622 (0.427788) with: {'activation': 'tanh'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid'}\n",
      "0.742956 (0.363516) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Cross validation code goes here\n",
    "def create_model(activation='relu'):\n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    "\n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "activation = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "\n",
    "kerasCl_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=kerasCl_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(train_images, train_labels)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analyze your results\n",
    "\n",
    "Does your choice of activation function matter?  When does the activation function perform best?  \n",
    "\n",
    "Things to consider:\n",
    "\n",
    "* Specifically does choosing the same activation function for all of the layers do best? \n",
    "* Does choosing different activation functions for each of the layers do best?\n",
    "* Are they all within the same approximate accuracy range?\n",
    "* do things vary wildly?\n",
    "\n",
    "Analysis: activation functions 'relu'  'sigmoid' and 'linear' are same here. it is noted that tanh is doing worse here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and explanation go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tune over more hyperparameters\n",
    "\n",
    "Now that we've tuned the activation functions, let's try tuning more parameters.  This time add tuning for the following parameters:\n",
    "\n",
    "* number of neurons per layer\n",
    "* weight initialization\n",
    "* optimizer\n",
    "* weight constraint\n",
    "* activation function\n",
    "* learning rate\n",
    "\n",
    "Here is a great post on the range of values you should consider: https://www.wandb.com/articles/fundamentals-of-neural-networks\n",
    "\n",
    "Here is some code that is also useful: https://www.kaggle.com/lavanyashukla01/training-a-neural-network-start-here\n",
    "\n",
    "for understanding this practically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5216/5216 [==============================] - 316s 61ms/step - loss: 4.1446 - accuracy: 0.7418\n",
      "Best: 0.742956 using {'activation': 'relu', 'dropout': 0.2, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.076289 (0.107889) with: {'activation': 'relu', 'dropout': 0.2, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.2, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.2, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.409622 (0.427788) with: {'activation': 'relu', 'dropout': 0.2, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.3, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.409622 (0.427788) with: {'activation': 'relu', 'dropout': 0.3, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.3, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.3, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.4, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.409622 (0.427788) with: {'activation': 'relu', 'dropout': 0.4, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.4, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.4, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.5, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.076289 (0.107889) with: {'activation': 'relu', 'dropout': 0.5, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.5, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.5, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.409622 (0.427788) with: {'activation': 'relu', 'dropout': 0.6, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.6, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.076289 (0.107889) with: {'activation': 'relu', 'dropout': 0.6, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.6, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.7, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.7, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.409622 (0.427788) with: {'activation': 'relu', 'dropout': 0.7, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.7, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.8, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.8, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.8, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'relu', 'dropout': 0.8, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.410006 (0.427421) with: {'activation': 'sigmoid', 'dropout': 0.2, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.2, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.2, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.2, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.409814 (0.427605) with: {'activation': 'sigmoid', 'dropout': 0.3, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.409622 (0.427788) with: {'activation': 'sigmoid', 'dropout': 0.3, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.409622 (0.427788) with: {'activation': 'sigmoid', 'dropout': 0.3, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.3, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742189 (0.362975) with: {'activation': 'sigmoid', 'dropout': 0.4, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.4, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.4, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.4, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.409622 (0.427788) with: {'activation': 'sigmoid', 'dropout': 0.5, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.5, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.5, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.689254 (0.332120) with: {'activation': 'sigmoid', 'dropout': 0.5, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.415760 (0.421960) with: {'activation': 'sigmoid', 'dropout': 0.6, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.6, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.6, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.6, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.409622 (0.427788) with: {'activation': 'sigmoid', 'dropout': 0.7, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.7, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.7, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.7, 'neurons': 1024, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.8, 'neurons': 512, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.8, 'neurons': 512, 'optimizer': 'Adam'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.8, 'neurons': 1024, 'optimizer': 'RMSprop'}\n",
      "0.742956 (0.363516) with: {'activation': 'sigmoid', 'dropout': 0.8, 'neurons': 1024, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# cross validation code goes here\n",
    "def create_model(activation='relu', dropout=0.5, optimizer='adam', neurons=1):\n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    "\n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(neurons, activation=activation))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "activation = ['relu',  'sigmoid']\n",
    "optimizer = ['RMSprop', 'Adam']\n",
    "neurons = [512, 1024]\n",
    "param_grid = dict(activation=activation, optimizer=optimizer, dropout=dropout, neurons= neurons)\n",
    "\n",
    "kerasCl_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=kerasCl_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(train_images, train_labels)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Tune over data augmentation\n",
    "\n",
    "Here you'll take the best hyperparameters from your neural network, with 4 trainable layers, and then add them to a pipeline.  We will then tune over data augmentation parameters.  Report out your mean and standard deviation of accuracy.\n",
    "\n",
    "Here we will create a scikit-learn pipline:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "If you need an example with gridsearch and pipeline:\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "\n",
    "As a reminder, here is the documentation for data augmentation:\n",
    "\n",
    "https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Analyze your results\n",
    "\n",
    "Now that you've tuned over model parameters and preprocessing, what has a bigger impact?  Why do you think that might be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using your best model and preprocessing to train a new model\n",
    "\n",
    "Now you should select the best hyperparameters for the neural network and the best hyperparameters for the preprocesser and then combine them into a scikit-learn pipeline.  Next train a classifier with these new tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 15,764,802\n",
      "Trainable params: 8,129,538\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimized_model = grid_result.best_estimator_.model\n",
    "optimized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4016 samples, validate on 1200 samples\n",
      "Epoch 1/5\n",
      "4016/4016 [==============================] - 292s 73ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 2/5\n",
      "4016/4016 [==============================] - 287s 71ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 3/5\n",
      "4016/4016 [==============================] - 285s 71ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 4/5\n",
      "4016/4016 [==============================] - 285s 71ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n",
      "Epoch 5/5\n",
      "4016/4016 [==============================] - 287s 71ms/step - loss: 4.0937 - accuracy: 0.7460 - val_loss: 4.3116 - val_accuracy: 0.7325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x4857f6f28>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifer generation code goes here\n",
    "X_train, X_val, y_train, y_val=train_test_split(train_images, train_labels, test_size=0.23, random_state=42)\n",
    "optimized_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Let's see if things improved - time for `classification_report`\n",
    "\n",
    "Now that you've tuned your model, let's see how well it does on our test set!  First call predict on the test data to get a prediction.  Then use `classification_report` to see how well the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.00      0.00      0.00       234\n",
      "   PNEUMONIA       0.62      1.00      0.77       390\n",
      "\n",
      "    accuracy                           0.62       624\n",
      "   macro avg       0.31      0.50      0.38       624\n",
      "weighted avg       0.39      0.62      0.48       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# prediction code goes here\n",
    "y_pred = optimized_model.predict(test_images)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(test_labels.argmax(axis=1), y_pred, target_names=LABELS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Analsis and comparison\n",
    "\n",
    "Now that you've seen how well your classifier does when it's been tuned, compare this with your previous model, that was untuned.  Are the precision, recall and f1-scores substantially different?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Prediction on COVID binary classification task with tuned model\n",
    "\n",
    "Now you'll use your tuned classifier to try and predict on the binary COVID19 case.  Please change the model to your tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       234\n",
      "           1       0.53      1.00      0.69       262\n",
      "\n",
      "    accuracy                           0.53       496\n",
      "   macro avg       0.26      0.50      0.35       496\n",
      "weighted avg       0.28      0.53      0.37       496\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayub/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#prediction code goes here\n",
    "\n",
    "# please make a copy of your tuned model and save it to variable:\n",
    "tuned_model = optimized_model\n",
    "\n",
    "# please specify the width and height you used for the image preprocessing\n",
    "width =64\n",
    "height = 64\n",
    "\n",
    "covid_features, covid_labels = extract_features_covid(tuned_model, width, height)\n",
    "non_covid_features, non_covid_labels = extract_features_not_covid(tuned_model, width, height)\n",
    "features =  np.concatenate((covid_features, non_covid_features), axis=0)\n",
    "labels =  np.concatenate((covid_labels, non_covid_labels), axis=0)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index, label in enumerate(labels):\n",
    "    if label == \"CLEAR TRAIN\":\n",
    "        X_train.append(features[index])\n",
    "        y_train.append(0)\n",
    "    if label == \"PNEUMONIA\":\n",
    "        X_train.append(features[index])\n",
    "        y_train.append(1)\n",
    "    if label == \"COVID\":\n",
    "        X_test.append(features[index])\n",
    "        y_test.append(1)\n",
    "    if label == \"CLEAR TEST\":\n",
    "        X_test.append(features[index])\n",
    "        y_test.append(0)\n",
    "\n",
    "logit_clf = LogisticRegression()\n",
    "logit_clf.fit(X_train, y_train)\n",
    "y_pred = logit_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Analyze your results\n",
    "\n",
    "Now that you've seen the results of your tuned model, compare those with the results of the untuned model.  Did things get better? Worse?  Why do you think this may or may not be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of your results goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is no improvement over untuned model. It looks both are suffering from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
